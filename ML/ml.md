# Machine Learning (ML)
> ìœ„ ë¬¸ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ (í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ + ë”¥ëŸ¬ë‹ ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ) ê³µë¶€í•œ ë‚´ìš©ì„ `TIL` ì±Œë¦°ì§€ë¥¼ ìœ„í•´ ì‘ì„±í•œ ë‚´ìš©ì…ë‹ˆë‹¤. 

---
> ë” ìì„¸í•œ ì½”ë“œ/ ì˜ˆì œëŠ” <ml>ì€ https://github.com/iam-Aerin/ml/tree/master [github]ì˜ repository ë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤. 

(ìˆ˜ì •í›„) **ë¨¸ì‹ ëŸ¬ë‹** (ì£¼ì œë³„) ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
(ìˆ˜ì •ì „) ~ì´ ë‚´ìš©ì€ ë‚ ì§œë³„ (í˜¹ì€ ì£¼ì°¨ë³„) TIL ê¸°ë¡ì„ ìœ„í•´ ìƒì„±í–ˆìŠµë‹ˆë‹¤.~

> TIL ë‚´ìš© ì •ë¦¬ (3ì›” 1ì£¼ì°¨) 
> 
> #ë‚´ë§˜ëŒ€ë¡œTILì±Œë¦°ì§€ #ë™ì•„ì¼ë³´ #ë¯¸ë””ì–´í”„ë¡ í‹°ì–´ #ê¸€ë¡œë²Œì†Œí”„íŠ¸ì›¨ì–´ìº í¼ìŠ¤ #GSCì‹ ì´Œ
`ê¸€ë¡œë²Œì†Œí”„íŠ¸ì›¨ì–´ìº í¼ìŠ¤ì™€ ë™ì•„ì¼ë³´ê°€ í•¨ê»˜ ì§„í–‰í•˜ëŠ” ì±Œë¦°ì§€ì…ë‹ˆë‹¤.`


TIL(Today I Learned) ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©ì„ ê¸°ë¡í•©ë‹ˆë‹¤.


### Links (ì´ë²ˆì£¼ í•™ìŠµì— ì‚¬ìš©ëœ ë§í¬ëª¨ìŒ)
- ml ê´€ë ¨ ë‚´ìš© ì™¸ì—ë„ ìµëª… ì§ˆë¬¸ ê²Œì‹œíŒì„ í†µí•´ 'ë°ì´í„° ë¶„ì„ê°€' í˜¹ì€ 'ê°œë°œì' ê´€ë ¨ ì»¤ë¦¬ì–´ ì§ˆì˜ì‘ë‹µ ì‹œê°„ì„ ê°€ì¡ŒìŒ.
---
- ë‚´ê°€ ê°€ê³  ì‹¶ì€ íšŒì‚¬, íŒ€ì„ ì •í•˜ì! ë¬´ì—‡ì„ í•„ìš”ë¡œ í•˜ëŠ”ì§€ â†’ ëª©í‘œë¥¼ ì¡ê³  ì–´ë–»ê²Œ ì¤€ë¹„í•´ ë‚˜ê°ˆì§€ ê°•ì‚¬ë‹˜ê»˜ ì—¬ì­¤ë³¼ ë‚´ìš©ì´ ìˆìœ¼ë©´, ì ê·¹ì ìœ¼ë¡œ ì—¬ì­¤ë³´ì.
---
- [ì „ ì„¸ê³„ ë””ìì´ë„ˆë“¤ ë‚œë¦¬ë‚œ í”¼ê·¸ë§ˆ ì»¨í¼ëŸ°ìŠ¤ ìš”ì•½ | Config2024](: AI í™œìš©) https://www.youtube.com/watch?v=aduVMrS-v4o
=> ë¬¸ì œ ì˜ì‹, ëª©ì ì„ ì•Œê³  ì¸ê³µì§€ëŠ¥ì„ ì‚¬ìš©í•  ì¤„ ì•Œì•„ì•¼ í•œë‹¤!


### keyword
> ì„ í˜•íšŒê·€, ë‹¤ì¤‘íšŒê·€, ë¦¿ì§€, ë¼ì˜, ë¡œì§€ìŠ¤í‹±íšŒê·€, ë‹¤ì¤‘íšŒê·€, ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜

> í•œ ì¼: 

- íŒŒì´ì¬ ì—°ìŠµì„ ìœ„í•´, í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ - ì…ë¬¸ ë¬¸ì œ í’€ì´ë¥¼ ì§„í–‰í•¨. (https://github.com/iam-Aerin/algo) ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨. 
- ì„ í˜•íšŒê·€ë¥¼ í†µí•´ `ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ëŒ€ë³€í•˜ëŠ” ìµœì ì˜ ì„ `ì„ ì°¾ì•„ë³´ì.

> ìì£¼ í‹€ë¦¬ëŠ” ë‚´ìš©!
- 


> ì–´ë ¤ì› ë˜ ì˜ˆì œ!
-

#
#
#

# ë¨¸ì‹ ëŸ¬ë‹ ....... 
## `git`ì˜ `ml` í´ë”ì—ì„œë„ í™•ì¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
#### https://github.com/iam-Aerin/ml/tree/master


#  MACHINE LEARNING (ml)
## ğŸ· ëª©ì°¨

### 1. K-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ (KNN Regression) (p.115)  
   - [`3-1.ipynb`](#)  

### 2. ì„ í˜• íšŒê·€ (Linear Regression) (p.130)  
   - [`3-2.ipynb`](#)  

### 3. íŠ¹ì„± ê³µí•™ê³¼ ê·œì œ (Feature Engineering & Regularization) (p.150)  
   - [`3-3.ipynb`](#)  

### 4. ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression) (p.176)  
   - [`4-1.ipynb`](#)  

### 5. íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ (Tree Algorithms) (p.220)  
   - [`5-1.ipynb`](#)  

### 6. ë¹„ì§€ë„ í•™ìŠµ (Unsupervised Learning) (p.286)  
   - [`6-1.ipynb`](#)  
   - **6-1. êµ°ì§‘ ì•Œê³ ë¦¬ì¦˜ (Clustering) (p.286)**  
   - **6-2. K-í‰ê·  (K-Means) (p.300)**  
     - [`6-2.ipynb`](#)  
   - **6-3. ì£¼ì„±ë¶„ ë¶„ì„ (PCA) (p.318)**  
     - [`6-3.ipynb`](#)  

### 7. ë”¥ëŸ¬ë‹ (Deep Learning) (p.339)  
   - [`7-1.ipynb`](#)  
   - **7-1. ì¸ê³µ ì‹ ê²½ë§ (Artificial Neural Networks) (p.339)**  
   - **7-2. ì‹¬ì¸µ ì‹ ê²½ë§ (Deep Neural Networks) (p.367)**  
     - [`7-2.ipynb`](#)  

### 8. í•©ì„±ê³± ì‹ ê²½ë§ (CNN) (p.422)  
   - [`8-1.ipynb`](#)  
   - **8-1. í•©ì„±ê³± (Convolution) (p.422)**  

# MACHINE LEARNING (ml)
### > `https://github.com/iam-Aerin/ml` ì— ë‚´ìš© ì •ë¦¬í•¨.
> #ì°¨íŠ¸ë¥¼ ì‹œê°í™”í•´ì„œ ì ì°ì–´ë³´ê¸°
- `#matplotlib library ë¥¼ ì„¤ì¹˜í•´ì„œ ì‹œê°í™”í•˜ê¸° - í„°ë¯¸ë„ì— (vscode)`

## KNN (K-Neareat Neighbours): K-ìµœê·¼ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜
- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
- SciKit ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ (vscodeì—ì„œ install ì„¤ì¹˜ë¥¼ í•˜ê³ ) -> ì•„ë˜ì™€ ê°™ì´ ë‚´ê°€ ì‚¬ìš©í• 
- `KNeighborsClassifier` ì•Œê³ ë¦¬ì¦˜ ê¸°ëŠ¥ì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•´ë³´ê² ë‹¤.

## 02-1. í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ (p.66)
- ì§€ë„í•™ìŠµìœ¼ë¡œ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ êµ¬ë¶„í•˜ì—¬ ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚´.
- í•™ìŠµ ì¸í’‹/ ì •ë‹µ ë°ì´í„° & í…ŒìŠ¤íŠ¸ ì¸í’‹/ ì •ë‹µ ë°ì´í„°ë¥¼ ë§Œë“¦. 

## 02-2 ë°ì´í„° ì „ì²˜ë¦¬ (p.87)
- í‘œì¤€ì ìˆ˜í™”í•´ì„œ ë°ì´í„°ë¥¼ í•™ìŠµ ì‹œí‚¤ê³ , í‰ê°€(í…ŒìŠ¤íŠ¸) í•œë‹¤.
- ì ìˆ˜ë¥¼ í‘œì¤€ì ìˆ˜ë¡œ ë°”ê¾¸ê² ë‹¤
- í‘œì¤€ì ìˆ˜ëŠ” ê° ë°ì´í„°ê°€ ì›ì ì—ì„œ ëª‡ í‘œì¤€í¸ì°¨ë§Œí¼ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤.
> ìì„¸í•œ ì˜ˆì œ ë¬¸ì œ ì½”ë“œëŠ” `https://github.com/iam-Aerin/ml/blob/master/2.2.ipynb` ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨.


# 1. k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ (K-Nearest Neighbors Regression)

## 1-1. íšŒê·€ ë¶„ì„ `KNeighborsRegressor`
- K-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€(KNN íšŒê·€)ëŠ” **ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ í‰ê· ì„ ê³„ì‚°**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ëª¨ë¸ì´ë‹¤.
- ì„ í˜•ì ì¸ ê´€ê³„ë¥¼ ê°€ì •í•˜ì§€ ì•Šê³ , ë°ì´í„° ë¶„í¬ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì¡°ì • ê°€ëŠ¥í•˜ë‹¤.

#### ğŸ“Œ `KNeighborsRegressor` ì‚¬ìš©ë²•
```python
from sklearn.neighbors import KNeighborsRegressor
knn_regressor = KNeighborsRegressor(n_neighbors=5)
knn_regressor.fit(X_train, y_train)
predictions = knn_regressor.predict(X_test)
```
- `n_neighbors`: ê³ ë ¤í•  ìµœê·¼ì ‘ ì´ì›ƒì˜ ê°œìˆ˜(K ê°’)
- `fit()`: í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ í›ˆë ¨
- `predict()`: ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡

## 1-2. ê³¼ëŒ€ì í•©ê³¼ ê³¼ì†Œì í•©

#### âœ… ê³¼ëŒ€ì í•© (Overfitting)
- K ê°’ì´ ë„ˆë¬´ ì‘ì„ ê²½ìš°(ì˜ˆ: K=1)
- ëª¨ë¸ì´ ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸ì— ë„ˆë¬´ ë¯¼ê°í•´ì ¸ì„œ í›ˆë ¨ ë°ì´í„°ì— ì™„ë²½íˆ ë§ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì¼ë°˜í™”ë˜ì§€ ì•ŠìŒ.

#### âœ… ê³¼ì†Œì í•© (Underfitting)
- K ê°’ì´ ë„ˆë¬´ í´ ê²½ìš°(ì˜ˆ: K=50)
- ëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ í‰ê· ì ì¸ ì˜ˆì¸¡ì„ í•˜ì—¬ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ ë°˜ì˜í•˜ì§€ ëª»í•¨.

#### ğŸ”¹ ìµœì ì˜ K ê°’ ì°¾ê¸°
- ì¼ë°˜ì ìœ¼ë¡œ **K=3~10** ì‚¬ì´ì—ì„œ ì ì ˆí•œ ê°’ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì„.
- **êµì°¨ ê²€ì¦(Cross Validation)** ì„ í™œìš©í•˜ì—¬ ìµœì ì˜ K ê°’ì„ ì°¾ì„ ìˆ˜ ìˆìŒ.

---
# ğŸ” K-ìµœê·¼ì ‘ ì´ì›ƒ(KNN) ì•Œê³ ë¦¬ì¦˜ vs K-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€(KNN íšŒê·€)

K-ìµœê·¼ì ‘ ì´ì›ƒ(KNN)ì€ **ë¶„ë¥˜(Classification)ì™€ íšŒê·€(Regression)ì—ì„œ ëª¨ë‘ ì‚¬ìš©**ë©ë‹ˆë‹¤.  
ë‘ ë°©ì‹ ëª¨ë‘ **ê°€ì¥ ê°€ê¹Œìš´ `k`ê°œì˜ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ì§€ë§Œ, **ì˜ˆì¸¡ ë°©ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤.**  

---

## ğŸ“Œ KNN ë¶„ë¥˜ (Classification)
- ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´, **ê°€ì¥ ê°€ê¹Œìš´ `k`ê°œì˜ ë°ì´í„° ì¤‘ ë‹¤ìˆ˜ê²° íˆ¬í‘œ**ë¥¼ í†µí•´ í´ë˜ìŠ¤ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.  
- **ì˜ˆì¸¡ ë°©ì‹:** `k`ê°œì˜ ì´ì›ƒ ì¤‘ **ê°€ì¥ ë§ì´ ë“±ì¥í•œ í´ë˜ìŠ¤(Label)ë¥¼ ì„ íƒ**  
- `k`ê°€ ì‘ìœ¼ë©´ â†’ **ê³¼ëŒ€ì í•© ìœ„í—˜** (í›ˆë ¨ ë°ì´í„°ì— ë„ˆë¬´ ë¯¼ê°)  
- `k`ê°€ í¬ë©´ â†’ **ê³¼ì†Œì í•© ìœ„í—˜** (ë„ˆë¬´ ë‹¨ìˆœí•œ ëª¨ë¸)  

### âœ… **ì˜ˆì œ (k=3)**  
```plaintext
ğŸ± ğŸ¶ ğŸ¶  â†’ ê³¼ë°˜ìˆ˜ê°€ ğŸ¶ â†’ ì˜ˆì¸¡ê°’: ğŸ¶


---
```python
from sklearn.model_selection import GridSearchCV
param_grid = {'n_neighbors': range(1, 20)}
grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
print("Best K value:", grid_search.best_params_['n_neighbors'])
```

ì´ë ‡ê²Œ ìµœì ì˜ K ê°’ì„ ì°¾ì•„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆë‹¤.


# 2. ì„ í˜• íšŒê·€ (Simple Linear Regression)
> ë…ë¦½ ë³€ìˆ˜ê°€ í•œ ê°œì¼ ë•Œ ì‚¬ìš© (ë‹¨ìˆœí•œ ì„ í˜• ê´€ê³„ë¥¼ ë¶„ì„)

```
ì˜ˆì œ:
 - í‚¤ì™€ ëª¸ë¬´ê²Œì˜ ê´€ê³„
 - ê³µë¶€ ì‹œê°„ê³¼ ì‹œí—˜ ì ìˆ˜ì˜ ê´€ê³„

 ì‚¬ìš©í•  ë•Œ:
 - í•œ ê°œì˜ ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•  ë•Œ
 ```


### âœ… ê°œë…
- ì„ í˜• íšŒê·€ëŠ” **ë…ë¦½ ë³€ìˆ˜(X)ì™€ ì¢…ì† ë³€ìˆ˜(Y) ê°„ì˜ ê´€ê³„ë¥¼ ì§ì„ (ì„ í˜• í•¨ìˆ˜)ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ê¸°ë²•**
- ì…ë ¥ ë³€ìˆ˜ì˜ ì„ í˜• ê²°í•©ì„ í†µí•´ ì¶œë ¥ì„ ì˜ˆì¸¡í•¨

### âœ… ì„ í˜• íšŒê·€ ê³µì‹

y = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚™Xâ‚™ + Îµ

- **Î²â‚€** : ì ˆí¸(intercept)
- **Î²â‚, Î²â‚‚, ..., Î²â‚™** : íšŒê·€ ê³„ìˆ˜(ê°€ì¤‘ì¹˜)
- **Xâ‚, Xâ‚‚, ..., Xâ‚™** : ë…ë¦½ ë³€ìˆ˜(ì„¤ëª… ë³€ìˆ˜)
- **Îµ** : ì˜¤ì°¨í•­(residual error)

### âœ… ì¢…ë¥˜
1. **ë‹¨ìˆœ ì„ í˜• íšŒê·€(Simple Linear Regression)**
## ë…ë¦½ ë³€ìˆ˜ê°€ **1ê°œ**ì¼ ë•Œ ì‚¬ìš©
   ```python
   from sklearn.linear_model import LinearRegression
   model = LinearRegression()
   model.fit(X_train, y_train)
   predictions = model.predict(X_test)
   ```

2. **ë‹¤ì¤‘ ì„ í˜• íšŒê·€(Multiple Linear Regression)**
 ## ë…ë¦½ ë³€ìˆ˜ê°€ **ì—¬ëŸ¬ ê°œ**ì¼ ë•Œ ì‚¬ìš©
   ```python
   model = LinearRegression()
   model.fit(X_train, y_train)
   ```

### âœ… ì¥ì 
- í•´ì„ì´ ê°„ë‹¨í•˜ê³  êµ¬í˜„ì´ ì‰¬ì›€
- ê³„ì‚°ëŸ‰ì´ ì ì–´ ë¹ ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥
- ê³¼ì í•©ì´ ì ìŒ (ë‹¨, ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆì„ ê²½ìš° ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)

### âœ… ë‹¨ì 
- **ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ê°€ ì„ í˜•ì´ì–´ì•¼ í•¨**
- ì´ìƒì¹˜(Outlier)ì— ë¯¼ê°í•¨
- ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ

---

## 2-2. ë‹¤í•­íšŒê·€ (Polynomial Regression)
### âœ… ê°œë…
- ë‹¤í•­ íšŒê·€ëŠ” **ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ê°€ ê³¡ì„ (ë¹„ì„ í˜•)ì¼ ë•Œ ì ìš©í•˜ëŠ” íšŒê·€ ê¸°ë²•**
- ì„ í˜• íšŒê·€ì˜ í™•ì¥ ê°œë…ì´ë©°, **ì…ë ¥ ë³€ìˆ˜ì˜ ê±°ë“­ì œê³± í•­(ì œê³±, ì„¸ì œê³± ë“±)ì„ ì¶”ê°€**í•˜ì—¬ ê³¡ì„ ì„ ëª¨ë¸ë§

### âœ… ë‹¤í•­ íšŒê·€ ê³µì‹

y = Î²â‚€ + Î²â‚X + Î²â‚‚XÂ² + Î²â‚ƒXÂ³ + ... + Î²â‚™Xâ¿ + Îµ

- **XÂ², XÂ³ ë“±ì˜ ê³ ì°¨í•­ì„ ì¶”ê°€í•˜ì—¬ ë¹„ì„ í˜• ê´€ê³„ë¥¼ í•™ìŠµ ê°€ëŠ¥**
- ì„ í˜• íšŒê·€ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê°€ì¤‘ì¹˜(Î²)ë¥¼ í•™ìŠµí•˜ì—¬ ìµœì ì˜ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±

### âœ… ì˜ˆì œ
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# 2ì°¨ ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„±
poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
poly_model.fit(X_train, y_train)
predictions = poly_model.predict(X_test)
```
- `PolynomialFeatures(degree=2)`: ì…ë ¥ ë³€ìˆ˜ì˜ 2ì°¨ í•­ì„ ì¶”ê°€
- `make_pipeline()`: ë‹¤í•­ ë³€í™˜ í›„ ì„ í˜• íšŒê·€ë¥¼ ì ìš©í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ìƒì„±

### âœ… ì¥ì 
- ì„ í˜• íšŒê·€ë³´ë‹¤ **ë¹„ì„ í˜• ë°ì´í„° í•™ìŠµì´ ê°€ëŠ¥**
- ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨í•œ ë³€í™˜ë§Œìœ¼ë¡œ ê³¡ì„  ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ

### âœ… ë‹¨ì 
- ì°¨ìˆ˜ë¥¼ ë„ˆë¬´ ë†’ì´ë©´ **ê³¼ì í•©(Overfitting)** ê°€ëŠ¥ì„± ì¦ê°€
- ë‹¤í•­ì‹ì˜ ì°¨ìˆ˜ë¥¼ ëª‡ ì°¨ê¹Œì§€ ì„¤ì •í• ì§€ ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”

---

# 3. íŠ¹ì„± ê³µí•™ê³¼ ê·œì œ

## 3-1. íŠ¹ì„±ê³µí•™ (Feature Engineering)
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ **ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë³€í˜•, ìƒì„±, ì¡°í•©í•˜ëŠ” ê³¼ì •**
- **ì¢‹ì€ íŠ¹ì„±ì„ ë§Œë“¤ì–´ì•¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒë¨**

#### âœ… íŠ¹ì„± ê³µí•™ì˜ ì£¼ìš” ê¸°ë²•
1. **ë‹¤í•­ íŠ¹ì„± ìƒì„±**
   - ê¸°ì¡´ íŠ¹ì„±ì„ ê±°ë“­ì œê³±í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ìƒì„±
   ```python
   from sklearn.preprocessing import PolynomialFeatures
   poly = PolynomialFeatures(degree=2, include_bias=False)
   X_poly = poly.fit_transform(X)
   ```

---

## 3-2. ê·œì œ (Regularization)
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ê³¼ì í•©ë˜ì§€ ì•Šë„ë¡ **ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œí•œí•˜ëŠ” ê¸°ë²•**
- ì„ í˜• íšŒê·€ ëª¨ë¸ì—ì„œ **íšŒê·€ ê³„ìˆ˜(ê°€ì¤‘ì¹˜)ë¥¼ ì œí•œí•˜ì—¬ ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ìœ ì§€**

#### âœ… ê·œì œ ì¢…ë¥˜: ë¦¿ì§€, ë¼ì˜

## 3-3. **ë¦¿ì§€ íšŒê·€ (Ridge Regression, L2 ì •ê·œí™”)**
   - íšŒê·€ ê³„ìˆ˜ì˜ ì œê³±í•©ì„ ìµœì†Œí™”í•˜ì—¬ ê³¼ì í•© ë°©ì§€
   ```python
   from sklearn.linear_model import Ridge
   ridge = Ridge(alpha=1.0)
   ridge.fit(X_train, y_train)
   ```

## 3-4. **ë¼ì˜ íšŒê·€ (Lasso Regression, L1 ì •ê·œí™”)**
   - ì¼ë¶€ íšŒê·€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ **ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ìë™ ì„ íƒ**
   ```python
   from sklearn.linear_model import Lasso
   lasso = Lasso(alpha=0.1)
   lasso.fit(X_train, y_train)
   ```

#### âœ… ê·œì œ ì‚¬ìš© ì‹œê¸°
- **ë¦¿ì§€ íšŒê·€**: ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆëŠ” ë°ì´í„°ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì‹¶ì„ ë•Œ
- **ë¼ì˜ íšŒê·€**: ì¤‘ìš”í•œ íŠ¹ì„±ë§Œ ìë™ ì„ íƒí•˜ê³  ì‹¶ì„ ë•Œ


>ì •ë¦¬: ì–¸ì œ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?
- ìƒí™©ë³„ ëª¨ë¸ ì¶”ì²œ
situation_model_mapping = {
    "ë…ë¦½ ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì´ê³ , ì„ í˜• ê´€ê³„ë¥¼ ê°€ì •": "ë‹¤ì¤‘ ì„ í˜• íšŒê·€",/
    "ë‹¤ì¤‘ê³µì„ ì„±ì´ ì¡´ì¬í•˜ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì‹¶ìŒ": "ë¦¿ì§€ íšŒê·€",/
    "ë…ë¦½ ë³€ìˆ˜ê°€ ë§ê³ , ì¤‘ìš” ë³€ìˆ˜ë§Œ ì„ íƒí•˜ê³  ì‹¶ìŒ": "ë¼ì˜ íšŒê·€",/
    "ë…ë¦½ ë³€ìˆ˜ê°€ í•œ ê°œë§Œ ìˆì„ ë•Œ": "ë‹¨ìˆœ ì„ í˜• íšŒê·€"
}

# ë¡œì§€ìŠ¤í‹± íšŒê·€ & íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜

# **4. ë¡œì§€ìŠ¤í‹± íšŒê·€ (p.176) `4-1.ipynb`**
ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” **ì„ í˜• íšŒê·€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ë¶„ë¥˜ ëª¨ë¸**ë¡œ, **ì´ì§„ ë¶„ë¥˜(Binary Classification)** ë° **ë‹¤ì¤‘ ë¶„ë¥˜(Multi-Class Classification)** ë¬¸ì œì— ì‚¬ìš©ë©ë‹ˆë‹¤.

---

## **4-1. ì´ì§„ ë¶„ë¥˜**
- **ì¶œë ¥ê°’ì´ 0 ë˜ëŠ” 1**ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜.
- ì…ë ¥ ë°ì´í„° \( X \)ë¥¼ í†µí•´ **ì„ í˜• íšŒê·€ ê°’**ì„ ê³„ì‚°í•œ í›„, **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid Function)**ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ê°’ì„ ì¶œë ¥.

---

## **4-2. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜**
- **ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜**ë¡œ, **ì¶œë ¥ì„ 0~1 ì‚¬ì´ì˜ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í™˜**í•¨.

- **íŠ¹ì§•:**
  - ì…ë ¥ ê°’ì´ **ì‘ì„ìˆ˜ë¡ 0ì— ê°€ê¹Œìš´ í™•ë¥ **, **í´ìˆ˜ë¡ 1ì— ê°€ê¹Œìš´ í™•ë¥ **.
  - **ì´ì§„ ë¶„ë¥˜ì—ì„œ í™•ë¥ ì  ì˜ˆì¸¡ì„ ìˆ˜í–‰**í•˜ëŠ” ë° ì‚¬ìš©ë¨.

---

## **4-3. ë‹¤ì¤‘ ë¶„ë¥˜**
- **3ê°œ ì´ìƒì˜ í´ë˜ìŠ¤**ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ í™•ì¥ ë²„ì „.
- ê°œë³„ í´ë˜ìŠ¤ì— ëŒ€í•œ **ì„ í˜• íšŒê·€ ê°’ì„ ê³„ì‚°í•œ í›„, ì†Œí”„íŠ¸ë§¥ìŠ¤(Softmax) í•¨ìˆ˜**ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ì„ êµ¬í•¨.

---

## **4-4. ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜**
- **ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” í™•ë¥  ë³€í™˜ í•¨ìˆ˜**ë¡œ, ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±.

- **íŠ¹ì§•:**
  - ëª¨ë“  í´ë˜ìŠ¤ì˜ í™•ë¥  ì´í•©ì´ **1ì´ ë˜ë„ë¡ ì •ê·œí™”**.
  - ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©.

---

## **4-5. í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (p.199) `4-2.ipynb`**
- ê²½ì‚¬ í•˜ê°•ë²•(GD)ì˜ í•œ í˜•íƒœë¡œ, **ëª¨ë“  ë°ì´í„°ê°€ ì•„ë‹Œ í•˜ë‚˜ì˜ ìƒ˜í”Œë§Œ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸**.
- ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜:
  - **ì´ì§„ ë¶„ë¥˜** â†’ **Binary Cross-Entropy (Log Loss)**
  - **ë‹¤ì¤‘ ë¶„ë¥˜** â†’ **Categorical Cross-Entropy**
- **ì¥ì :** ë¹ ë¥¸ í•™ìŠµ ì†ë„, ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥.
- **ë‹¨ì :** ê¸°ìš¸ê¸°ê°€ ë¶ˆì•ˆì •í•˜ì—¬ ìˆ˜ë ´ ì†ë„ê°€ ë¶ˆê·œì¹™í•  ìˆ˜ ìˆìŒ.

---

# **5. íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ (p.220) `5-1.ipynb`**
íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ **ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë¹„ì„ í˜• ëª¨ë¸**ë¡œ, ê²°ì • íŠ¸ë¦¬(Decision Tree)ì™€ ì•™ìƒë¸” í•™ìŠµ(ëœë¤ í¬ë ˆìŠ¤íŠ¸, ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë“±)ì— ì‚¬ìš©ë¨.

---

## **5-1. ê²°ì • íŠ¸ë¦¬**
#### âœ… **(1) ë¶ˆìˆœë„**
- **íŠ¸ë¦¬ê°€ ë°ì´í„°ë¥¼ ë¶„í• í•˜ëŠ” ê¸°ì¤€**.
- ëŒ€í‘œì ì¸ ë¶ˆìˆœë„ ì¸¡ì • ë°©ë²•:
  - **ì§€ë‹ˆ ë¶ˆìˆœë„ (Gini Impurity)**

#### âœ… **(2) ê°€ì§€ì¹˜ê¸° (Pruning)**
- **íŠ¸ë¦¬ê°€ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€**í•˜ê¸° ìœ„í•œ ë°©ë²•.
- **ì‚¬ì „ ê°€ì§€ì¹˜ê¸°(Pre-Pruning)**: ìµœëŒ€ ê¹Šì´ ì œí•œ, ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì œí•œ ì ìš©.
- **ì‚¬í›„ ê°€ì§€ì¹˜ê¸°(Post-Pruning)**: í•™ìŠµ í›„ í•„ìš” ì—†ëŠ” ë…¸ë“œë¥¼ ì œê±°.

---

## **5-2. êµì°¨ ê²€ì¦ê³¼ ê·¸ë¦¬ë“œ ì„œì¹˜ (p.242) `5-2.ipynb`**
#### âœ… **(1) ê²€ì¦ ì„¸íŠ¸**
- **ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ì„¸íŠ¸**.
- ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ **í›ˆë ¨ ì„¸íŠ¸ì™€ ë³„ë„ë¡œ ìœ ì§€**.

#### âœ… **(2) êµì°¨ ê²€ì¦ (`cross_validate()`)**
- ë°ì´í„°ë¥¼ **Kê°œ(ì˜ˆ: 5-fold)ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ í‰ê°€**í•˜ëŠ” ê¸°ë²•.
- **ì¼ë°˜ì ì¸ K-fold êµì°¨ ê²€ì¦ ê³¼ì •**
  1. ë°ì´í„°ë¥¼ **Kê°œë¡œ ë¶„í• **.
  2. **K-1ê°œì˜ ì„¸íŠ¸ë¡œ í›ˆë ¨**, ë‚˜ë¨¸ì§€ 1ê°œë¡œ ê²€ì¦.
  3. **ì´ ê³¼ì •ì„ Kë²ˆ ë°˜ë³µí•˜ì—¬ í‰ê·  ì„±ëŠ¥ì„ ê³„ì‚°**.

```python
from sklearn.model_selection import cross_validate
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
scores = cross_validate(dt, train_input, train_target, cv=5, return_train_score=True)
print(scores['test_score'].mean())  # í‰ê·  í…ŒìŠ¤íŠ¸ ì ìˆ˜ ì¶œë ¥
```

## **5-3. íŠ¸ë¦¬ì˜ ì•™ìƒë¸”**
- ëœë¤ í¬ë ˆìŠ¤íŠ¸
> í˜„ì‹œì ì—ì„œ ê°€ì¥ ë³´í¸ì ìœ¼ë¡œ, ì„±ëŠ¥ì´ ë†’ë‹¤ê³  ì•Œë ¤ì§„ ì•Œê³ ë¦¬ì¦˜.
>
> ì •í˜• ë°ì´í„° ì—ì„œ e.g. í‘œ êµ¬ì¡°ì˜ ìˆ«ì ë°ì´í„° (csv) í˜¹ì€ ì—‘ì…€

`ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°`
- ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§: ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ë°©ì‹
`RandomForestRegressor()`

ëœë¤í¬ë ˆìŠ¤íŠ¸ëŠ” ê²°ì •íŠ¸ë¦¬ì˜ ì•™ìƒë¸”ì´ê¸° ë–„ë¬¸ì—
DecisionTreeClassifier ê°€ ì œê³µí•˜ëŠ” ì¤‘ìš”í•œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì œê³µí•¨.
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

####  ëœë¤í¬ë ˆìŠ¤íŠ¸ ì‹¤í–‰
`RandomForestClassifier`

```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=1)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=1)
print(scores)
```
 cross_validateëŠ” êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.
 return_train_score=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ í›ˆë ¨ ì„¸íŠ¸ ì ìˆ˜ë„ í•¨ê»˜ ì œê³µëœë‹¤.
 ê²°ê³¼ëŠ” fit_time, score_time, test_score, train_score ë“±ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ëœë‹¤.

 ğŸ’¡ ì´ ì½”ë“œì˜ ëª©ì ì€ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒ!




#### ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬
- ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬ëŠ” ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ìœ ì‚¬í•˜ì§€ë§Œ, ë…¸ë“œë¥¼ ë¶„í• í•  ë•Œ ìµœì ì˜ ë¶„í• ì„ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¬´ì‘ìœ„ë¡œ ë¶„í• í•˜ëŠ” ë°©ì‹ì´ë‹¤.
```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=1)

scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=1)
print(scores)
```

- ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… (Gradient Boosting)

ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì•™ìƒë¸”ì„ êµ¬ì„±í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ê³¼ëŒ€ì í•©ì— ê°•í•˜ê³  ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤.
```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_validate

gb = GradientBoostingClassifier(n_estimators=500)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=1)
print(scores)
```

---

## 7. ë”¥ëŸ¬ë‹ (Deep Learning) (p.339)

### âœ… ë”¥ëŸ¬ë‹ì´ë€?

ë”¥ëŸ¬ë‹(Deep Learning)ì€ ì¸ê³µ ì‹ ê²½ë§ì˜ ì¸µ(layer)ì„ ê¹Šê²Œ ìŒ“ì•„ ì˜¬ë ¤ ë³µì¡í•œ ë°ì´í„°ì˜ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì´ë‹¤.

- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë‹¬ë¦¬ **íŠ¹ì„±(feature)ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œ**í•¨
- **ë¹„ì„ í˜• ë¬¸ì œ** ë° **ë³µì¡í•œ ë°ì´í„° ì²˜ë¦¬**ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„
- ëŒ€í‘œì ì¸ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬: **TensorFlow, PyTorch**

---

## 7-1. ì¸ê³µ ì‹ ê²½ë§ (Artificial Neural Networks, ANN) (p.339)  
ğŸ“‚ [`7-1.ipynb`](#)

### âœ… ê°œë…
- ì‚¬ëŒì˜ ë‡Œ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œ ì˜ê°ì„ ë°›ì•„ ê°œë°œí•œ ì•Œê³ ë¦¬ì¦˜
- ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ í•™ìŠµ

### âœ… ì£¼ìš” í‚¤ì›Œë“œ
- **ë‰´ëŸ°(Neuron)**: ì‹ ê²½ë§ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ ì…ë ¥ê°’ì„ ë°›ì•„ ì²˜ë¦¬í•˜ëŠ” ê³„ì‚° ë‹¨ìœ„
- **í™œì„±í™” í•¨ìˆ˜(Activation function)**: ë‰´ëŸ°ì´ ì¶œë ¥ê°’ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
  - `Sigmoid`: ì¶œë ¥ê°’ì„ 0~1 ì‚¬ì´ë¡œ ë³€í™˜ (ì´ì§„ ë¶„ë¥˜ì—ì„œ ì‚¬ìš©)
  - **ReLU(ë ë£¨)**: ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¡œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë¨
- **ì†ì‹¤í•¨ìˆ˜(Loss Function)**: ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ì‚¬ì´ ì˜¤ì°¨ë¥¼ ì¸¡ì • (ì˜ˆ: MSE, Cross-Entropy)
- **ì˜µí‹°ë§ˆì´ì €(Optimizer)**: ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë„¤íŠ¸ì›Œí¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ (ì˜ˆ: SGD, Adam)

### âœ… TensorFlowë¡œ ANN ëª¨ë¸ êµ¬ì¶• ì˜ˆì œ
```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # ì´ì§„ë¶„ë¥˜ ì˜ˆì‹œ
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32)

---
## 7-2. ì‹¬ì¸µ ì‹ ê²½ë§ (Deep Neural Networks, DNN)

ì¸ê³µ ì‹ ê²½ë§(ANN)ì— ì€ë‹‰ì¸µ(hidden layer)ì„ ì—¬ëŸ¬ ê°œ ì¶”ê°€í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ê¹Šê²Œ ë§Œë“  í˜•íƒœì´ë‹¤.  
ë³µì¡í•œ ë°ì´í„°ì˜ íŠ¹ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

### âœ… ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ (Activation Functions)

- **ì‹œê·¸ëª¨ì´ë“œ(sigmoid)**  
  - ì¶œë ¥ê°’ì„ 0ê³¼ 1 ì‚¬ì´ë¡œ ë³€í™˜í•˜ì—¬ í™•ë¥ ë¡œ í‘œí˜„
  - ì£¼ë¡œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì‚¬ìš©

- **ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax)**  
  - ì—¬ëŸ¬ í´ë˜ìŠ¤ ê°„ì˜ ë¶„ë¥˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì‚¬ìš©
  - í´ë˜ìŠ¤ë³„ í™•ë¥ ì˜ ì´í•©ì€ í•­ìƒ 1

- **ë ë£¨(ReLU)**  
  - ìŒìˆ˜ ê°’ì€ 0, ì–‘ìˆ˜ ê°’ì€ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
  - ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ ë°©ì§€í•˜ì—¬ ê¹Šì€ ì‹ ê²½ë§ì—ì„œ ë„ë¦¬ ì‚¬ìš©

### âœ… ì‹¬ì¸µ ì‹ ê²½ë§ ì˜ˆì‹œ (TensorFlow Keras)

```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
---

# 8. í•©ì„±ê³± ì‹ ê²½ë§ (Convolutional Neural Networks, CNN)

í•©ì„±ê³± ì‹ ê²½ë§(CNN, Convolutional Neural Networks)ì€ ì´ë¯¸ì§€ ë˜ëŠ” ì˜ìƒê³¼ ê°™ì€ ê³µê°„ì  íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ íš¨ê³¼ì ì¸ ì‹ ê²½ë§ êµ¬ì¡°ì´ë‹¤.

## âœ… CNNì˜ í•µì‹¬ ê°œë…

### ğŸ“Œ í•©ì„±ê³±(Convolution)
- ì´ë¯¸ì§€ë¥¼ í•„í„°(ì»¤ë„)ë¡œ í›‘ì–´ê°€ë©° íŠ¹ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •
- ì´ë¯¸ì§€ì˜ ì§€ì—­ì  í”½ì…€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

### ğŸ“Œ í•©ì„±ê³±ì¸µ(Convolution Layer)
- ì—¬ëŸ¬ ê°œì˜ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì–‘í•œ íŠ¹ì§•(feature)ì„ ì¶”ì¶œí•œë‹¤.
- í•„í„°ì˜ ê°œìˆ˜ë§Œí¼ ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§• ë§µ(feature map)ì´ ìƒì„±ëœë‹¤.

### ğŸ“Œ í’€ë§ì¸µ(Pooling Layer)
- íŠ¹ì§• ë§µ(feature map)ì˜ í¬ê¸°ë¥¼ ì¶•ì†Œí•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ê°ì†Œì‹œí‚¤ê³ , ì¤‘ìš”í•œ íŠ¹ì§•ë§Œì„ ìœ ì§€í•œë‹¤.
- ëŒ€í‘œì ì¸ ë°©ì‹: MaxPooling(ìµœëŒ€ í’€ë§), AveragePooling(í‰ê·  í’€ë§)

### ğŸ“Œ íŒ¨ë”©(Padding)
- ì…ë ¥ ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ì— ê°€ìƒì˜ ë°ì´í„°(ì£¼ë¡œ 0)ë¥¼ ì¶”ê°€í•˜ì—¬, í•©ì„±ê³± ì—°ì‚° í›„ì—ë„ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ìœ ì§€í•˜ê³  ê°€ì¥ìë¦¬ ì •ë³´ì˜ ì†ì‹¤ì„ ë°©ì§€í•œë‹¤.

---

âœ… **ì¶”ê°€ ì˜ˆì œ ì½”ë“œ (TensorFlow Keras)**  
```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)),
    keras.layers.MaxPooling2D(pool_size=(2,2)),
    keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2,2)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])